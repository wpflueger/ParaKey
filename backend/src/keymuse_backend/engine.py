"""Inference engine for speech-to-text transcription.

This module provides both mock and real inference engines for the KeyMuse
backend service.
"""

from __future__ import annotations

import asyncio
import logging
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Iterable, Optional

from keymuse_backend.config import BackendConfig

logger = logging.getLogger(__name__)


@dataclass(frozen=True)
class EngineEvent:
    """An event generated by the inference engine."""

    kind: str  # 'partial', 'final', 'status', 'error'
    text: str
    stability: float | None = None


class InferenceEngine:
    """Speech-to-text inference engine using NeMo Parakeet model.

    This engine handles real ASR inference using the NVIDIA Parakeet TDT
    model. It uses a thread pool executor to run blocking inference calls
    without blocking the async event loop.
    """

    def __init__(self, config: BackendConfig) -> None:
        """Initialize the inference engine.

        Args:
            config: Backend configuration.
        """
        self._config = config
        self._model_loader = None
        self._executor = ThreadPoolExecutor(max_workers=1)
        self._loaded = False

    @property
    def is_loaded(self) -> bool:
        """Return True if the model is loaded."""
        return self._loaded

    @property
    def device(self) -> str:
        """Return the device being used for inference."""
        if self._model_loader:
            return self._model_loader.device
        return "unknown"

    def load_model(self) -> None:
        """Load the ASR model.

        This should be called on server startup.
        """
        if self._loaded:
            logger.info("Model already loaded")
            return

        try:
            from keymuse_backend.model import ModelLoader

            self._model_loader = ModelLoader(
                model_name=self._config.model_name,
                device=self._config.device,
            )
            self._model_loader.load()
            self._loaded = True
            logger.info(f"Inference engine ready on {self._model_loader.device}")

        except Exception as e:
            logger.error(f"Failed to load model: {e}")
            raise

    def unload_model(self) -> None:
        """Unload the model and free resources."""
        if self._model_loader:
            self._model_loader.unload()
            self._model_loader = None
        self._loaded = False
        logger.info("Model unloaded")

    def _transcribe_sync(self, audio_data: bytes, sample_rate: int) -> str:
        """Synchronously transcribe audio (runs in thread pool).

        Args:
            audio_data: Raw PCM audio bytes.
            sample_rate: Sample rate of the audio.

        Returns:
            Transcribed text.
        """
        if not self._loaded or self._model_loader is None:
            raise RuntimeError("Model not loaded")

        return self._model_loader.transcribe(audio_data, sample_rate)

    async def transcribe(
        self,
        audio_data: bytes,
        sample_rate: int = 16000,
    ) -> str:
        """Asynchronously transcribe audio.

        Args:
            audio_data: Raw PCM audio bytes (16-bit signed).
            sample_rate: Sample rate of the audio.

        Returns:
            Transcribed text.
        """
        if not self._loaded:
            raise RuntimeError("Model not loaded - call load_model() first")

        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self._executor,
            self._transcribe_sync,
            audio_data,
            sample_rate,
        )

    async def process_audio_stream(
        self,
        audio_frames: list[bytes],
        sample_rate: int = 16000,
    ) -> list[EngineEvent]:
        """Process a stream of audio frames and generate events.

        This concatenates all audio frames and performs a single
        transcription, generating partial events during processing
        and a final event with the result.

        Args:
            audio_frames: List of audio frame bytes.
            sample_rate: Sample rate of the audio.

        Returns:
            List of engine events.
        """
        events: list[EngineEvent] = []

        # Generate periodic partial events while processing
        partial_interval = self._config.partial_every_n_frames
        for i, _ in enumerate(audio_frames, start=1):
            if i % partial_interval == 0:
                events.append(
                    EngineEvent(
                        kind="partial",
                        text="Processing...",
                        stability=0.3,
                    )
                )

        # Concatenate all audio frames
        audio_data = b"".join(audio_frames)

        if len(audio_data) == 0:
            events.append(EngineEvent(kind="final", text=""))
            return events

        try:
            # Perform transcription
            transcript = await self.transcribe(audio_data, sample_rate)

            # Add final event
            events.append(EngineEvent(kind="final", text=transcript.strip()))

        except RuntimeError as e:
            error_str = str(e).lower()
            if "cuda" in error_str and ("out of memory" in error_str or "oom" in error_str):
                logger.error("CUDA out of memory during transcription")
                events.append(
                    EngineEvent(
                        kind="error",
                        text="Transcription failed: CUDA out of memory",
                    )
                )
            else:
                logger.error(f"Transcription failed: {e}")
                events.append(
                    EngineEvent(
                        kind="error",
                        text=f"Transcription failed: {e}",
                    )
                )

        except Exception as e:
            logger.error(f"Transcription failed: {e}")
            events.append(
                EngineEvent(
                    kind="error",
                    text=f"Transcription failed: {e}",
                )
            )

        return events


class MockInferenceEngine:
    """Mock inference engine for testing without the real model."""

    def __init__(self, config: BackendConfig) -> None:
        self._config = config
        self._loaded = False

    @property
    def is_loaded(self) -> bool:
        return self._loaded

    @property
    def device(self) -> str:
        return "mock"

    def load_model(self) -> None:
        self._loaded = True
        logger.info("Mock inference engine loaded")

    def unload_model(self) -> None:
        self._loaded = False
        logger.info("Mock inference engine unloaded")

    async def transcribe(
        self,
        audio_data: bytes,
        sample_rate: int = 16000,
    ) -> str:
        # Simulate processing time
        await asyncio.sleep(0.1)
        return self._config.final_text

    async def process_audio_stream(
        self,
        audio_frames: list[bytes],
        sample_rate: int = 16000,
    ) -> list[EngineEvent]:
        events: list[EngineEvent] = []

        # Generate partial events
        for i, _ in enumerate(audio_frames, start=1):
            if i % self._config.partial_every_n_frames == 0:
                events.append(
                    EngineEvent(
                        kind="partial",
                        text="Listening...",
                        stability=0.4,
                    )
                )

        # Final event with mock text
        events.append(EngineEvent(kind="final", text=self._config.final_text))

        return events


def create_engine(config: BackendConfig) -> InferenceEngine | MockInferenceEngine:
    """Create an inference engine based on configuration.

    Args:
        config: Backend configuration.

    Returns:
        An inference engine instance.
    """
    if config.mode == "mock":
        return MockInferenceEngine(config)
    return InferenceEngine(config)


# Legacy function for backwards compatibility
def generate_mock_events(
    config: BackendConfig, audio_frames: Iterable[bytes]
) -> list[EngineEvent]:
    """Generate mock events (backwards compatibility).

    Args:
        config: Backend configuration.
        audio_frames: Iterable of audio frame bytes.

    Returns:
        List of mock engine events.
    """
    events: list[EngineEvent] = []
    for index, _frame in enumerate(audio_frames, start=1):
        if index % config.partial_every_n_frames == 0:
            events.append(
                EngineEvent(kind="partial", text="Listening...", stability=0.4)
            )
    events.append(EngineEvent(kind="final", text=config.final_text))
    return events


__all__ = [
    "EngineEvent",
    "InferenceEngine",
    "MockInferenceEngine",
    "create_engine",
    "generate_mock_events",
]
